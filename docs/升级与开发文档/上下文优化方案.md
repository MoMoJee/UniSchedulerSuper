# Agent 上下文优化方案

## 1. 问题分析

### 1.1 当前状态
- **完整历史保存**: LangGraph checkpoint 中保存所有消息
- **无优化机制**: 每次调用 LLM 时发送所有历史消息
- **模型硬编码**: DeepSeek 配置直接写在代码中，无法切换
- **潜在风险**: 
  - 长对话会话可能超过模型上下文窗口（DeepSeek 128K tokens）
  - Token 消耗随消息数量线性增长
  - 工具输出可能包含大量冗余信息
  - 无 Token 消耗统计，无法控制成本

### 1.2 设计目标
1. **保持完整性**: Checkpoint 中保存完整的对话历史（用于回滚、审计）
2. **优化发送**: 在调用 LLM 前动态优化消息列表
3. **透明性**: 用户无感知，不影响正常使用
4. **用户配置化**: 
   - 支持系统预置和用户自定义模型
   - 模型配置存储在 `core.UserData` 中
   - 前端界面进行配置，而不是 Django Admin
5. **Token 统计**: 
   - 实时收集 Token 消耗量和费用
   - 按模型、按日期统计
   - 存储在 `core.UserData` 中

## 2. 优化策略

### 2.1 基于 Token 的动态窗口管理

#### 核心理念
不使用固定的消息数量，而是基于 **Token 消耗** 和 **模型窗口占比** 动态调整：

```
模型窗口大小: 128,000 tokens (可配置，支持模型切换)
目标使用率: 60% = 76,800 tokens
分配策略:
  - System Prompt: ~2,000 tokens (3%)
  - 历史总结: ~20,000 tokens (26%, 可配置)
  - 最近对话: ~50,000 tokens (65%, 可配置)
  - 预留空间: ~6,800 tokens (9%, 用于 LLM 响应)
```

#### 消息分层（三层结构）

```
┌─────────────────────────────────────────────────┐
│ Layer 1: System Prompt                          │
│ - 用户偏好、对话风格、工具列表                      │
│ - Token: ~2,000 (固定)                           │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ Layer 2: 历史总结 (Summarized History)           │
│ - 旧消息的智能总结                                 │
│ - Token: 可配置 (默认 20,000)                     │
│ - 触发条件: 当新消息 > 总结部分的 50%              │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ Layer 3: 最近对话 (Recent Context)               │
│ - 最近的 X 组对话 (动态计算)                       │
│ - Token: 可配置 (默认 50,000)                     │
│ - 保持完整性，不压缩                               │
└─────────────────────────────────────────────────┘
```

### 2.2 智能总结策略（核心实施）

#### 工作流程

```
完整历史: [M1, M2, ..., M80, M81, ..., M98, M99, M100]
              ↓
检测 Token 使用:
  - M1-M80: 60,000 tokens (已总结)
  - M81-M100: 30,000 tokens (最近对话)
  - 总计: 90,000 tokens > 目标 76,800 tokens

触发总结条件:
  - 新增消息 (M81-M100) Token 数 = 30,000
  - 历史总结 (Summary of M1-M80) = 20,000
  - 比例: 30,000 / 20,000 = 150% > 阈值 50%
  → 触发重新总结

执行总结:
  - 调用 LLM 总结 M1-M100
  - 新总结 Token: ~25,000
  - 保留 M101+ 作为最近对话
  
发送给 LLM:
  [System] + [Summary(M1-M100)] + [M101, M102, ...]
```

#### 总结提示词模板

```python
summarize_prompt = f"""
请总结以下对话历史，保留关键信息：

## 对话历史 (共 {message_count} 条消息, {history_tokens} tokens)
{conversation_text}

## 总结要求
1. 提取用户的主要意图和需求
2. 记录重要的决策和操作结果
3. 保留关键的时间、地点、人物信息
4. 压缩冗余的工具调用细节
5. 目标长度: {target_summary_tokens} tokens

## 输出格式
简洁的段落式总结，保持时间顺序。
"""
```

### 2.3 Token 计算策略

#### 数据来源（优先级）

1. **LangGraph 返回的实际统计** (最优)
   ```python
   # LangGraph 响应中包含 usage_metadata
   response = llm_with_tools.invoke(messages)
   usage = response.response_metadata.get('usage', {})
   prompt_tokens = usage.get('prompt_tokens', 0)
   completion_tokens = usage.get('completion_tokens', 0)
   ```

2. **tiktoken 精确计算** (备选)
   ```python
   import tiktoken
   encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
   tokens = len(encoding.encode(text))
   ```

3. **粗略估算** (兜底)
   ```python
   # 1 token ≈ 2.5 字符（中英文混合）
   estimated_tokens = len(text) / 2.5
   ```

#### Token 追踪

在每次 LLM 调用后记录：
```python
class TokenUsageTracker:
    def __init__(self):
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.call_count = 0
    
    def record(self, prompt_tokens, completion_tokens):
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.call_count += 1
```

### 2.4 触发条件（基于 Token）

```python
# 条件 1: 总 Token 数超过目标窗口的目标占比
current_tokens = calculate_total_tokens(messages)
target_tokens = model_window_size * target_usage_ratio  # 例如: 128000 * 0.6
if current_tokens > target_tokens:
    messages = optimize_messages(messages)

# 条件 2: 新消息累积超过历史总结的阈值比例
if has_summary:
    new_messages_tokens = calculate_tokens(messages_after_summary)
    summary_tokens = calculate_tokens(summary)
    ratio = new_messages_tokens / summary_tokens
    if ratio > summary_trigger_ratio:  # 例如: 0.5 (50%)
        trigger_re_summarize()
```

### 2.5 工具输出压缩（辅助优化）

在计算 Token 时，工具输出也会被压缩以节省空间：

```python
原始 ToolMessage (2000+ tokens):
  content: """
  {
    "items": [
      {"id": "xxx", "title": "会议", "start": "2026-01-01T10:00", ...},
      {"id": "yyy", "title": "复习", "start": "2026-01-01T14:00", ...},
      ...（50个事件）
    ],
    "total": 50
  }
  """

压缩后 (50 tokens):
  content: "查询成功: #1 会议(2026-01-01T10:00), #2 复习 ...共50项"
```

## 3. 实现方案

### 3.1 架构设计

```
┌─────────────────────────────────────────────────┐
│ Checkpoint (完整历史 + 总结元数据)                 │
│                                                 │
│ messages: [M1, M2, ..., M98, M99, M100]        │
│ summary_metadata: {                            │
│   "summary": "...",                            │
│   "summarized_until": 80,                      │
│   "summary_tokens": 20000,                     │
│   "created_at": "2026-01-01T10:00:00"          │
│ }                                              │
└────────────────┬────────────────────────────────┘
                 │ 读取
                 ↓
┌─────────────────────────────────────────────────┐
│ agent_node                                      │
│                                                 │
│  1. 读取完整历史                                  │
│     messages = state['messages']                │
│     summary_meta = state.get('summary_meta')    │
│                                                 │
│  2. 计算 Token 使用                              │
│     tokens = calculate_tokens_from_metadata()   │
│     ├─ 优先使用 LangGraph 返回的实际统计          │
│     └─ 备选使用 tiktoken 或估算                  │
│                                                 │
│  3. 检查是否需要总结                              │
│     if should_summarize(tokens, summary_meta):  │
│         summary_meta = summarize_history()      │
│                                                 │
│  4. 构建优化的上下文                              │
│     optimized = build_context(                  │
│         system_prompt,                          │
│         summary_meta['summary'],                │
│         recent_messages                         │
│     )                                           │
│                                                 │
│  5. 调用 LLM                                     │
│     response = llm.invoke(optimized)            │
│     record_token_usage(response.usage)          │
│                                                 │
│  6. 更新 summary_metadata (如果有总结)           │
│     return {                                    │
│         "messages": [response],                 │
│         "summary_metadata": summary_meta        │
│     }                                           │
└─────────────────────────────────────────────────┘
```

**关键点**:
1. **完整历史保存**: Checkpoint 保存所有消息 + 总结元数据
2. **动态计算**: 每次基于 Token 数量重新计算保留哪些消息
3. **增量总结**: 只在需要时重新总结，避免重复计算
4. **元数据追踪**: 记录总结到哪里、Token 数、创建时间

### 3.2 配置参数

#### 存储位置

配置参数存储在 `core.UserData` 的 JSON 字段中，而不是 `agent_service.DialogStyle`：

```python
# core/models.py
class UserData(models.Model):
    # 模型配置
    agent_model_config = models.JSONField(default=dict)
    # 结构: {
    #   "current_model_id": "system_deepseek",
    #   "models": {
    #     "system_deepseek": {
    #       "name": "DeepSeek Chat",
    #       "context_window": 128000,
    #       "cost_per_1k_input": 0.00014,
    #       "cost_per_1k_output": 0.00028
    #     }
    #   }
    # }
    
    # 上下文优化配置
    context_optimization_config = models.JSONField(default=dict)
    # 结构: {
    #   "enable_optimization": True,
    #   "target_usage_ratio": 0.6,
    #   "summary_token_ratio": 0.26,
    #   "recent_token_ratio": 0.65,
    #   ...
    # }
    
    # Token 消耗统计
    token_usage_stats = models.JSONField(default=dict)
    # 结构: {
    #   "total_input_tokens": 1500000,
    #   "total_output_tokens": 500000,
    #   "total_cost": 12.50,
    #   "quota": 9999999,
    #   "daily_stats": {...},
    #   "model_stats": {...}
    # }
```

#### 配置参数详解

```python
class ContextOptimizationConfig:
    """上下文优化配置（存储在 UserData.context_optimization_config）"""
    
    # 总开关
    enable_optimization: bool = True            # 是否启用优化
    
    # Token 分配策略
    target_usage_ratio: float = 0.6             # 目标使用率 (60%)
    summary_token_ratio: float = 0.26           # 历史总结占比 (26%)
    recent_token_ratio: float = 0.65            # 最近对话占比 (65%)
    
    # 总结触发条件
    enable_summarization: bool = True           # 是否启用智能总结
    summary_trigger_ratio: float = 0.5          # 触发重新总结的阈值 (50%)
    min_messages_before_summary: int = 20       # 最少消息数才开始总结
    
    # 工具输出压缩
    compress_tool_output: bool = True           # 是否压缩工具输出
    tool_output_max_tokens: int = 200           # 工具输出最大 Token 数
    
    # Token 计算方式
    token_calculation_method: str = "actual"    # actual / tiktoken / estimate
    
    # 注意：model_window_size 从模型配置中读取
    # 每个模型有自己的 context_window 值
```

#### 配置来源

```python
def get_optimization_config(user):
    """获取用户的优化配置"""
    user_data = user.user_data
    
    # 1. 模型配置（包含窗口大小）
    model_config = user_data.get_current_model_config()
    model_window = model_config['context_window']  # 128000 for DeepSeek
    
    # 2. 优化配置
    opt_config = user_data.get_optimization_config()
    
    # 3. 计算 Token 预算
    target_tokens = int(model_window * opt_config['target_usage_ratio'])
    summary_budget = int(target_tokens * opt_config['summary_token_ratio'])
    recent_budget = int(target_tokens * opt_config['recent_token_ratio'])
    
    return {
        'model_window': model_window,
        'target_tokens': target_tokens,
        'summary_budget': summary_budget,
        'recent_budget': recent_budget,
        **opt_config
    }
```

### 3.3 核心函数

#### 3.3.1 Token 计算

```python
class TokenCalculator:
    """Token 计算器，支持多种计算方式"""
    
    def __init__(self, method: str = "actual"):
        self.method = method
        self.encoding = None
        if method == "tiktoken":
            import tiktoken
            self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    
    def calculate_from_messages(
        self, 
        messages: List[BaseMessage],
        usage_metadata: Optional[Dict] = None
    ) -> int:
        """
        计算消息列表的 Token 数
        
        Args:
            messages: 消息列表
            usage_metadata: LangGraph 返回的实际使用统计（优先）
            
        Returns:
            Token 数量
        """
        # 方法 1: 使用实际统计（最准确）
        if self.method == "actual" and usage_metadata:
            return usage_metadata.get('prompt_tokens', 0)
        
        # 方法 2: 使用 tiktoken（精确）
        if self.method == "tiktoken" and self.encoding:
            total = 0
            for msg in messages:
                content = self._get_message_content(msg)
                total += len(self.encoding.encode(content))
            return total
        
        # 方法 3: 粗略估算（兜底）
        total_chars = 0
        for msg in messages:
            content = self._get_message_content(msg)
            total_chars += len(content)
        return int(total_chars / 2.5)
    
    def _get_message_content(self, msg: BaseMessage) -> str:
        """提取消息内容"""
        content = getattr(msg, 'content', '')
        if isinstance(content, list):
            return ' '.join(str(item) for item in content)
        return str(content)
```

#### 3.3.2 总结判断与执行

```python
def should_summarize(
    messages: List[BaseMessage],
    summary_metadata: Optional[Dict],
    config: ContextOptimizationConfig,
    token_calculator: TokenCalculator
) -> bool:
    """
    判断是否需要触发总结
    
    条件:
    1. 消息数量足够多（> min_messages_before_summary）
    2. 新增消息 Token 数 > 历史总结的 summary_trigger_ratio
    """
    if not config.enable_summarization:
        return False
    
    if len(messages) < config.min_messages_before_summary:
        return False
    
    # 如果还没有总结，检查是否需要首次总结
    if not summary_metadata:
        total_tokens = token_calculator.calculate_from_messages(messages)
        return total_tokens > config.target_total_tokens * 0.8
    
    # 如果已有总结，检查新增消息是否超过阈值
    summarized_until = summary_metadata.get('summarized_until', 0)
    summary_tokens = summary_metadata.get('summary_tokens', 0)
    
    new_messages = messages[summarized_until:]
    new_tokens = token_calculator.calculate_from_messages(new_messages)
    
    ratio = new_tokens / summary_tokens if summary_tokens > 0 else float('inf')
    
    logger.info(
        f"[总结检查] 新消息: {len(new_messages)} 条, {new_tokens} tokens, "
        f"历史总结: {summary_tokens} tokens, 比例: {ratio:.1%}"
    )
    
    return ratio > config.summary_trigger_ratio


async def summarize_messages(
    messages: List[BaseMessage],
    llm,
    config: ContextOptimizationConfig,
    token_calculator: TokenCalculator
) -> Dict:
    """
    总结消息历史
    
    Returns:
        {
            "summary": "总结文本",
            "summarized_until": 消息索引,
            "summary_tokens": Token 数,
            "created_at": 创建时间
        }
    """
    # 构建总结提示
    conversation_text = _format_messages_for_summary(messages)
    current_tokens = token_calculator.calculate_from_messages(messages)
    
    summarize_prompt = f"""
请总结以下对话历史，保留关键信息：

## 对话历史
共 {len(messages)} 条消息，约 {current_tokens} tokens

{conversation_text}

## 总结要求
1. 提取用户的主要意图和需求
2. 记录重要的决策和操作结果
3. 保留关键的时间、地点、人物信息
4. 压缩冗余的工具调用细节
5. 目标长度: 约 {config.summary_token_budget} tokens

## 输出格式
简洁的段落式总结，保持时间顺序，重点突出。
"""
    
    # 调用 LLM 生成总结
    response = await llm.ainvoke([HumanMessage(content=summarize_prompt)])
    summary_text = response.content
    summary_tokens = token_calculator.calculate_from_messages([response])
    
    logger.info(
        f"[总结完成] 原始: {len(messages)} 条消息 ({current_tokens} tokens), "
        f"总结: {summary_tokens} tokens, "
        f"压缩率: {(1 - summary_tokens/current_tokens)*100:.1f}%"
    )
    
    return {
        "summary": summary_text,
        "summarized_until": len(messages),
        "summary_tokens": summary_tokens,
        "created_at": datetime.datetime.now().isoformat()
    }
```

#### 3.3.3 构建优化上下文

```python
def build_optimized_context(
    system_prompt: str,
    messages: List[BaseMessage],
    summary_metadata: Optional[Dict],
    config: ContextOptimizationConfig,
    token_calculator: TokenCalculator
) -> List[BaseMessage]:
    """
    构建优化后的上下文
    
    结构: [SystemMessage] + [Summary (可选)] + [Recent Messages]
    """
    optimized = []
    
    # 1. System Prompt
    system_msg = SystemMessage(content=system_prompt)
    optimized.append(system_msg)
    system_tokens = token_calculator.calculate_from_messages([system_msg])
    
    # 2. 历史总结（如果有）
    summary_tokens = 0
    recent_start_index = 0
    
    if summary_metadata:
        summary_text = summary_metadata.get('summary', '')
        if summary_text:
            summary_msg = SystemMessage(content=f"## 对话历史总结\n{summary_text}")
            optimized.append(summary_msg)
            summary_tokens = summary_metadata.get('summary_tokens', 0)
            recent_start_index = summary_metadata.get('summarized_until', 0)
    
    # 3. 最近对话（动态计算保留多少）
    recent_messages = messages[recent_start_index:]
    
    # 计算可用的 Token 预算
    available_tokens = (
        config.target_total_tokens 
        - system_tokens 
        - summary_tokens 
        - config.response_token_budget
    )
    
    # 从后往前累加消息，直到达到预算
    selected_messages = []
    cumulative_tokens = 0
    
    for msg in reversed(recent_messages):
        # 压缩工具输出
        if config.compress_tool_output and isinstance(msg, ToolMessage):
            msg = compress_tool_message(msg, config.tool_output_max_tokens)
        
        msg_tokens = token_calculator.calculate_from_messages([msg])
        
        if cumulative_tokens + msg_tokens > available_tokens:
            break
        
        selected_messages.insert(0, msg)
        cumulative_tokens += msg_tokens
    
    optimized.extend(selected_messages)
    
    # 日志
    logger.info(
        f"[上下文构建] System: {system_tokens}t, "
        f"Summary: {summary_tokens}t, "
        f"Recent: {cumulative_tokens}t ({len(selected_messages)} 条), "
        f"总计: {system_tokens + summary_tokens + cumulative_tokens}t / "
        f"{config.target_total_tokens}t"
    )
    
    return optimized
```

## 4. 实施计划

### Phase 1: 基础 Token 计算和追踪（立即实施）
**目标**: 建立 Token 统计基础设施

- [ ] 创建 `TokenCalculator` 类
  - 支持从 LangGraph response_metadata 读取实际 Token
  - 集成 tiktoken 库（可选）
  - 实现粗略估算兜底
- [ ] 在 `agent_node` 中记录每次调用的 Token 使用
- [ ] 添加 Token 使用日志和监控

**预期效果**:
- 可准确追踪每次对话的 Token 消耗
- 为后续优化提供数据基础

**验收标准**:
- 日志中显示每次调用的 prompt_tokens 和 completion_tokens
- Token 计算误差 < 10%

---

### Phase 2: 模型配置系统（短期，重要）
**目标**: 将模型配置迁移到 UserData，支持用户自定义模型和 Token 统计

#### 2.1 模型配置存储

- [ ] 修改 `core.UserData` 模型：
  - 添加 `agent_model_config` JSON 字段（模型列表和当前选择）
  - 添加 `context_optimization_config` JSON 字段（优化参数）
  - 添加 `token_usage_stats` JSON 字段（消耗统计）
- [ ] 实现 `get_current_model_config()` 方法
- [ ] 实现 `update_token_usage()` 方法
- [ ] 创建数据库迁移

#### 2.2 系统预置模型

- [ ] 在 `core/views.py` 中定义系统模型列表
  - DeepSeek Chat（默认）
  - 可扩展：GPT-4, Claude, GLM 等
- [ ] 每个模型包含：
  - 名称、API URL、窗口大小
  - 输入输出价格
  - 是否支持工具调用

#### 2.3 前端配置 API

- [ ] 创建 API 端点：
  - `GET /api/agent/model-config/` - 获取模型配置
  - `POST /api/agent/model-config/update/` - 更新配置
  - `GET /api/agent/token-usage/` - 获取统计
- [ ] 支持操作：
  - 选择当前模型
  - 添加/删除自定义模型
  - 更新优化参数

#### 2.4 前端 AI 设置界面

- [ ] 修改 `core/templates/ai_settings.html`：
  - 模型选择和管理区域
  - Token 统计展示（今天/7天/全部）
  - 上下文优化配置表单
- [ ] JavaScript 交互逻辑
- [ ] 实时刷新 Token 统计

**预期效果**:
- 用户可在前端切换模型
- 用户可添加自己的 API Key
- 实时查看 Token 消耗和费用

---

### Phase 3: Token 统计与监控（短期）
**目标**: 实时收集和展示 Token 消耗

- [ ] 在 `agent_node` 中集成 Token 统计：
  ```python
  usage = response.response_metadata.get('usage', {})
  user_data.update_token_usage(
      input_tokens=usage['prompt_tokens'],
      output_tokens=usage['completion_tokens'],
      model_id=current_model_id,
      cost=calculated_cost
  )
  ```
- [ ] 按维度统计：
  - 累计总量
  - 每日统计
  - 每个模型统计
- [ ] 前端展示：
  - 统计卡片（输入/输出/费用/剩余）
  - 模型使用对比表格
  - 时间趋势图（可选）

**预期效果**:
- 用户可查看实时消耗
- 为未来的限额功能打基础

---

### Phase 4: 智能总结机制（核心，中期）
**目标**: 实现对话历史的智能总结

- [ ] 实现 `should_summarize()` 判断逻辑
- [ ] 实现 `summarize_messages()` 总结函数
- [ ] 在 `AgentState` 中添加 `summary_metadata` 字段
- [ ] 修改 `agent_node` 集成总结流程
- [ ] 设计总结提示词模板（可配置）
- [ ] 测试总结质量和压缩率

**预期效果**:
- 旧对话自动总结，Token 使用稳定
- 总结触发阈值可配置（默认 50%）
- 总结质量保持 AI 理解上下文的能力

**验收标准**:
- 100 条消息的对话能压缩到目标 Token 范围
- AI 仍能理解对话历史中的关键信息
- 总结生成耗时 < 5 秒

---

### Phase 4: 动态上下文构建（核心，中期）
**目标**: 基于 Token 预算动态选择消息

- [ ] 实现 `build_optimized_context()` 函数
- [ ] 从后往前累加消息直到达到预算
- [ ] 集成工具输出压缩
- [ ] 处理边界情况（消息过大、总结失败等）

**预期效果**:
- 发送给 LLM 的 Token 数稳定在目标范围
- 消息选择智能化，保留最相关的内容

**验收标准**:
- Token 使用率在目标 ±5% 范围内
- 最近对话优先保留
- 工具输出压缩有效

---

### Phase 5: 监控和调优（长期）
**目标**: 持续优化效果

- [ ] 添加总结质量评估机制
- [ ] A/B 测试不同配置的效果
- [ ] 前端展示 Token 使用统计
- [ ] 用户反馈收集
- [ ] 根据实际使用调整默认参数

**关键指标**:
- Token 节省率（目标 50-70%）
- 对话质量评分（不低于基线）
- 总结生成成功率（> 95%）
- 用户满意度

## 5. 监控指标

### 5.1 Token 使用指标

| 指标 | 说明 | 目标值 |
|------|------|--------|
| **Prompt Tokens** | 每次调用发送给 LLM 的 Token 数 | < 76,800 (60% of 128K) |
| **Token 使用率** | prompt_tokens / model_window_size | 50-60% |
| **Token 节省率** | (优化前 - 优化后) / 优化前 × 100% | 40-70% |
| **累计消耗** | UserData.token_usage_stats.total_input_tokens | 实时统计 |
| **每日消耗** | 按日期统计的 Token 数 | 趋势分析 |
| **模型消耗** | 按模型统计的 Token 数 | 对比不同模型 |

### 5.2 成本指标

| 指标 | 说明 | 监控点 |
|------|------|--------|
| **累计费用** | UserData.token_usage_stats.total_cost | 总支出（美元）|
| **每日费用** | daily_stats[date]['cost'] | 每日趋势 |
| **模型费用** | model_stats[model_id]['cost'] | 各模型对比 |
| **平均费用** | 每次对话的平均成本 | 成本优化 |
| **预估剩余额度** | quota - total_tokens | 预警提示 |

### 5.3 用户配置指标

| 指标 | 说明 | 数据来源 |
|------|------|--------|
| **系统模型使用率** | 使用系统提供模型的用户比例 | agent_model_config |
| **自定义模型数量** | 用户添加的自定义模型平均数 | agent_model_config.models |
| **优化启用率** | 启用上下文优化的用户比例 | context_optimization_config |
| **配置修改频率** | 用户修改配置的频率 | 更新日志 |
| **平均 Token/消息** | prompt_tokens / message_count | 监控趋势 |

### 5.2 总结质量指标

| 指标 | 说明 | 目标值 |
|------|------|--------|
| **压缩率** | summary_tokens / original_tokens × 100% | 20-40% |
| **总结触发频率** | 每次对话触发总结的次数 | 监控趋势 |
| **总结耗时** | 生成总结的时间 | < 5 秒 |
| **总结成功率** | 成功生成总结的比例 | > 95% |

### 5.3 性能指标

| 指标 | 说明 | 目标值 |
|------|------|--------|
| **优化耗时** | 上下文优化的计算时间 | < 100ms |
| **LLM 响应时间** | 首 Token 延迟 | 监控变化 |
| **内存使用** | 历史消息 + 总结的内存占用 | 监控增长 |

### 5.4 日志示例

```
[上下文优化] Session: user_1_abc123
[Token 计算] 方法: actual (LangGraph metadata)
[Token 计算] 完整历史: 85 条消息, 估算 92,000 tokens
[总结检查] 新消息: 25 条, 35,000 tokens, 历史总结: 18,000 tokens, 比例: 194% > 阈值 50%
[总结触发] 开始总结 85 条消息...
[总结完成] 原始: 85 条 (92,000 tokens), 总结: 22,000 tokens, 压缩率: 76.1%
[上下文构建] System: 1,800t, Summary: 22,000t, Recent: 48,500t (18 条), 总计: 72,300t / 76,800t
[Token 节省] 优化前: 92,000t → 优化后: 72,300t, 节省: 21.4%
[性能] 优化耗时: 85ms, 总结耗时: 3.2s
```

### 5.5 数据库监控表（可选）

```python
class TokenUsageLog(models.Model):
    """Token 使用日志"""
    session = models.ForeignKey(AgentSession, on_delete=models.CASCADE)
    timestamp = models.DateTimeField(auto_now_add=True)
    
    # Token 统计
    prompt_tokens = models.IntegerField()
    completion_tokens = models.IntegerField()
    total_tokens = models.IntegerField()
    
    # 优化效果
    original_tokens = models.IntegerField(null=True)  # 优化前
    optimized_tokens = models.IntegerField(null=True)  # 优化后
    savings_ratio = models.FloatField(null=True)  # 节省率
    
    # 上下文组成
    system_tokens = models.IntegerField(null=True)
    summary_tokens = models.IntegerField(null=True)
    recent_tokens = models.IntegerField(null=True)
    message_count = models.IntegerField()
    
    # 总结信息
    has_summary = models.BooleanField(default=False)
    summary_triggered = models.BooleanField(default=False)
    
    # 性能指标
    optimization_time_ms = models.IntegerField(null=True)
    summarization_time_ms = models.IntegerField(null=True)
```

## 6. 风险评估

### 6.1 潜在风险
1. **上下文丢失**: 截断过多可能导致 AI 遗忘早期信息
   - **缓解**: 保留足够的窗口大小（20 条）
   - **备选**: 提示用户"如需参考更早的对话，请手动回顾"

2. **工具输出理解**: 压缩可能影响 AI 理解工具返回的数据
   - **缓解**: 保留关键字段（id, title, time）
   - **测试**: 充分测试各工具压缩后的效果

3. **多轮工具调用**: AI 可能需要引用之前工具的完整输出
   - **缓解**: 保留最近的工具输出完整性
   - **策略**: 只压缩 N 条之前的工具输出

### 6.2 回退方案
- 如果检测到 AI 回复质量下降，可临时禁用优化
- 提供 API 手动刷新完整上下文

## 7. 成功标准

### 7.1 功能验证
- ✅ 长对话（100+ 消息）不会导致上下文溢出
- ✅ AI 回复质量不下降
- ✅ 工具调用仍然准确

### 7.2 性能指标
- ✅ Token 消耗降低 40-60%
- ✅ 响应速度提升或持平
- ✅ 优化耗时 < 50ms

### 7.3 用户体验
- ✅ 用户无感知
- ✅ 对话连贯性保持
- ✅ 无额外操作负担
