"""
å†²çªåˆ†ææœåŠ¡
æä¾›æ—¥ç¨‹å†²çªæ£€æµ‹å’Œ LLM æ™ºèƒ½åˆ†æåŠŸèƒ½
"""
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Tuple
from logger import logger


# ==========================================
# æ—¥æœŸæ—¶é—´è§£æå·¥å…·
# ==========================================

def parse_datetime(dt_str: str) -> Optional[datetime]:
    """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
    if not dt_str:
        return None
    
    # æ”¯æŒå¤šç§æ ¼å¼
    formats = [
        '%Y-%m-%dT%H:%M:%S',
        '%Y-%m-%dT%H:%M:%S.%f',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%dT%H:%M:%S.%fZ',
        '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M',  # ISO 8601 without seconds
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d %H:%M',
        '%Y-%m-%d',
    ]
    
    for fmt in formats:
        try:
            return datetime.strptime(dt_str.replace('+00:00', 'Z').rstrip('Z'), fmt.rstrip('Z'))
        except ValueError:
            continue
    
    logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ—¶é—´: {dt_str}")
    return None


def format_datetime_for_display(dt: datetime) -> str:
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´ç”¨äºæ˜¾ç¤º"""
    return dt.strftime('%Y-%m-%d %H:%M')


def get_weekday_name(dt: datetime) -> str:
    """è·å–æ˜ŸæœŸåç§°"""
    weekdays = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
    return weekdays[dt.weekday()]


# ==========================================
# ç¡¬å†²çªæ£€æµ‹ç®—æ³•
# ==========================================

def detect_hard_conflicts(events: List[dict]) -> List[dict]:
    """
    æ£€æµ‹æ—¶é—´é‡å çš„ç¡¬å†²çªï¼ˆçº¯ç®—æ³•ï¼Œä¸ä¾èµ– LLMï¼‰
    
    å‰¯ä½œç”¨ï¼šä¼šåœ¨æˆåŠŸè§£æçš„äº‹ä»¶ä¸Šæ·»åŠ  _parsed_start å’Œ _parsed_end å­—æ®µ
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« id, title, start, end ç­‰å­—æ®µ
    
    Returns:
        å†²çªåˆ—è¡¨ï¼Œæ¯ä¸ªå†²çªåŒ…å«:
        - conflict_id: å†²çªç¼–å·
        - events: å†²çªçš„ä¸¤ä¸ªäº‹ä»¶
        - overlap_duration: é‡å æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
        - overlap_period: é‡å æ—¶æ®µ
    """
    if len(events) < 2:
        return []
    
    # è§£ææ—¶é—´ï¼Œå¹¶ç›´æ¥ä¿®æ”¹åŸå§‹äº‹ä»¶ï¼ˆæ·»åŠ  _parsed_* å­—æ®µï¼‰
    parsed_events = []
    for event in events:
        # è·³è¿‡å·²è§£æçš„äº‹ä»¶
        if '_parsed_start' in event and '_parsed_end' in event:
            parsed_events.append(event)
            continue
            
        start = parse_datetime(event.get('start', ''))
        end = parse_datetime(event.get('end', ''))
        
        if start and end and end > start:
            # ç›´æ¥ä¿®æ”¹åŸå§‹äº‹ä»¶ï¼Œæ·»åŠ è§£æç»“æœ
            event['_parsed_start'] = start
            event['_parsed_end'] = end
            parsed_events.append(event)
    
    # æŒ‰å¼€å§‹æ—¶é—´æ’åº
    sorted_events = sorted(parsed_events, key=lambda e: e['_parsed_start'])
    
    conflicts = []
    conflict_id = 1
    
    for i, event1 in enumerate(sorted_events):
        start1 = event1['_parsed_start']
        end1 = event1['_parsed_end']
        
        for event2 in sorted_events[i + 1:]:
            start2 = event2['_parsed_start']
            end2 = event2['_parsed_end']
            
            # æ£€æŸ¥æ—¶é—´é‡å 
            if start2 < end1:
                overlap_start = max(start1, start2)
                overlap_end = min(end1, end2)
                overlap_minutes = int((overlap_end - overlap_start).total_seconds() / 60)
                
                if overlap_minutes > 0:
                    conflicts.append({
                        'conflict_id': conflict_id,
                        'events': [
                            {k: v for k, v in event1.items() if not k.startswith('_parsed')},
                            {k: v for k, v in event2.items() if not k.startswith('_parsed')}
                        ],
                        'overlap_duration': overlap_minutes,
                        'overlap_period': {
                            'start': format_datetime_for_display(overlap_start),
                            'end': format_datetime_for_display(overlap_end)
                        }
                    })
                    conflict_id += 1
            else:
                # å·²æ’åºï¼Œåé¢çš„ä¸ä¼šå†é‡å 
                break
    
    return conflicts


# ==========================================
# å·¥ä½œå¯†åº¦åˆ†æ
# ==========================================

def analyze_daily_density(events: List[dict]) -> Dict[str, dict]:
    """
    åˆ†ææ¯æ—¥å·¥ä½œå¯†åº¦
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
    
    Returns:
        æŒ‰æ—¥æœŸåˆ†ç»„çš„å¯†åº¦åˆ†æç»“æœ
    """
    # æŒ‰æ—¥æœŸåˆ†ç»„
    daily_events: Dict[str, List[dict]] = {}
    
    for event in events:
        # ä¼˜å…ˆä½¿ç”¨å·²è§£æçš„æ—¶é—´ï¼ˆé¿å…é‡å¤è§£æï¼‰
        if '_parsed_start' in event:
            start = event['_parsed_start']
        else:
            start = parse_datetime(event.get('start', ''))
            
        if start:
            date_str = start.strftime('%Y-%m-%d')
            if date_str not in daily_events:
                daily_events[date_str] = []
            daily_events[date_str].append(event)
    
    # åˆ†ææ¯ä¸€å¤©
    results = {}
    for date_str, day_events in daily_events.items():
        total_minutes = 0
        event_count = len(day_events)
        
        for event in day_events:
            # ä¼˜å…ˆä½¿ç”¨å·²è§£æçš„æ—¶é—´
            if '_parsed_start' in event and '_parsed_end' in event:
                start = event['_parsed_start']
                end = event['_parsed_end']
            else:
                start = parse_datetime(event.get('start', ''))
                end = parse_datetime(event.get('end', ''))
                
            if start and end:
                duration = (end - start).total_seconds() / 60
                total_minutes += duration
        
        total_hours = total_minutes / 60
        date_obj = datetime.strptime(date_str, '%Y-%m-%d')
        weekday = get_weekday_name(date_obj)
        
        # è¯„ä¼°è´Ÿè½½
        if total_hours > 10:
            load_level = 'overload'
            load_emoji = 'ğŸ”´'
            load_text = 'è¿‡è½½'
        elif total_hours > 8:
            load_level = 'high'
            load_emoji = 'ğŸŸ '
            load_text = 'è¾ƒé«˜'
        elif total_hours > 6:
            load_level = 'normal'
            load_emoji = 'ğŸŸ¢'
            load_text = 'æ­£å¸¸'
        else:
            load_level = 'light'
            load_emoji = 'ğŸ”µ'
            load_text = 'è½»æ¾'
        
        results[date_str] = {
            'date': date_str,
            'weekday': weekday,
            'event_count': event_count,
            'total_hours': round(total_hours, 1),
            'load_level': load_level,
            'load_emoji': load_emoji,
            'load_text': load_text,
            'events': day_events
        }
    
    return results


# ==========================================
# è·å–ç”¨æˆ·ä¸ªäººåå¥½
# ==========================================

def get_user_personal_info(user) -> List[dict]:
    """è·å–ç”¨æˆ·ä¸ªäººåå¥½æ•°æ®"""
    from agent_service.models import UserPersonalInfo
    
    try:
        personal_info_qs = UserPersonalInfo.objects.filter(
            user=user
        ).order_by('key')
        
        return [
            {
                'key': info.key,
                'value': info.value,
                'description': info.description or ''
            }
            for info in personal_info_qs
        ]
    except Exception as e:
        logger.error(f"è·å–ä¸ªäººåå¥½å¤±è´¥: {e}")
        return []


# ==========================================
# æ„å»º LLM åˆ†ææç¤ºè¯
# ==========================================

def build_analysis_prompt(
    events: List[dict],
    hard_conflicts: List[dict],
    personal_info: List[dict],
    daily_density: Dict[str, dict],
    analysis_focus: List[str]
) -> str:
    """æ„å»ºè¯¦ç»†çš„åˆ†ææç¤ºè¯"""
    
    prompt_parts = []
    
    # 1. ç”¨æˆ·ä¸ªäººåå¥½
    prompt_parts.append("# ç”¨æˆ·ä¸ªäººä¿¡æ¯ä¸åå¥½\n")
    if personal_info:
        for info in personal_info:
            desc_part = f" ({info['description']})" if info['description'] else ""
            prompt_parts.append(f"- **{info['key']}**: {info['value']}{desc_part}")
    else:
        prompt_parts.append("ï¼ˆæš‚æ— ä¸ªäººåå¥½æ•°æ®ï¼Œè¯·åŸºäºå¸¸è§„æ ‡å‡†åˆ†æï¼‰")
    
    # 2. æ—¶é—´æ®µå†…æ‰€æœ‰æ—¥ç¨‹ï¼ˆåˆ†ç±»æ˜¾ç¤ºï¼‰
    prompt_parts.append("\n\n# å¾…åˆ†æçš„æ—¥ç¨‹å®‰æ’\n")
    
    # åˆ†ç¦»ç”¨æˆ·è‡ªå·±çš„æ—¥ç¨‹å’Œåˆ«äººçš„æ—¥ç¨‹
    user_events = [e for e in events if e.get('_source') == 'user']
    others_events = [e for e in events if e.get('_source') == 'share_group']
    
    logger.info(f"[å†²çªåˆ†æ] æ„å»ºæç¤ºè¯: æ€»å…± {len(events)} ä¸ªæ—¥ç¨‹, ç”¨æˆ·æ—¥ç¨‹ {len(user_events)} ä¸ª, åˆ†äº«ç»„æ—¥ç¨‹ {len(others_events)} ä¸ª")
    
    if user_events:
        prompt_parts.append("## ç”¨æˆ·è‡ªå·±çš„æ—¥ç¨‹ï¼ˆå¯ç¼–è¾‘ï¼‰\n")
        for event in user_events:
            event_str = _format_event_for_analysis(event)
            prompt_parts.append(event_str)
    
    if others_events:
        prompt_parts.append("\n## åˆ†äº«ç»„ä¸­ä»–äººçš„æ—¥ç¨‹ï¼ˆåªè¯»ï¼‰\n")
        for event in others_events:
            event_str = _format_event_for_analysis(event)
            prompt_parts.append(event_str)
    
    # 3. ç¡¬å†²çªåˆ—è¡¨
    prompt_parts.append("\n\n# æ£€æµ‹åˆ°çš„ç¡¬å†²çªï¼ˆæ—¶é—´é‡å ï¼‰\n")
    if hard_conflicts:
        for conflict in hard_conflicts:
            e1, e2 = conflict['events']
            prompt_parts.append(
                f"**å†²çª {conflict['conflict_id']}**: "
                f"#{e1.get('_index', '?')} ã€Š{e1.get('title', 'æœªå‘½å')}ã€‹ ä¸ "
                f"#{e2.get('_index', '?')} ã€Š{e2.get('title', 'æœªå‘½å')}ã€‹ "
                f"åœ¨ {conflict['overlap_period']['start']} ~ {conflict['overlap_period']['end']} "
                f"é‡å  {conflict['overlap_duration']} åˆ†é’Ÿ"
            )
    else:
        prompt_parts.append("âœ… æœªæ£€æµ‹åˆ°æ—¶é—´é‡å ")
    
    # 4. å·¥ä½œå¯†åº¦é¢„åˆ†æ
    prompt_parts.append("\n\n# æ¯æ—¥å·¥ä½œå¯†åº¦ç»Ÿè®¡\n")
    for date_str, density in sorted(daily_density.items()):
        prompt_parts.append(
            f"- **{date_str} ({density['weekday']})**: "
            f"{density['event_count']} ä¸ªäº‹ä»¶ï¼Œæ€»æ—¶é•¿ {density['total_hours']} å°æ—¶ "
            f"{density['load_emoji']} {density['load_text']}"
        )
    
    # 5. åˆ†æä»»åŠ¡
    prompt_parts.append("\n\n# åˆ†æä»»åŠ¡\n")
    prompt_parts.append("è¯·æ ¹æ®ç”¨æˆ·çš„æ—¥ç¨‹å®‰æ’å’Œä¸ªäººåå¥½ï¼Œè¿›è¡Œä»¥ä¸‹åˆ†æï¼š\n")
    
    if 'conflicts' in analysis_focus:
        prompt_parts.append("""
## 1. å†²çªçœŸå®æ€§åˆ¤æ–­
å¯¹äºæ¯ä¸ªç¡¬å†²çªï¼Œè¯·åˆ¤æ–­ï¼š
- **æ˜¯å¦çœŸçš„å†²çª**ï¼šæœ‰äº›äº‹æƒ…å¯ä»¥åŒæ—¶è¿›è¡Œï¼ˆå¦‚å¬éŸ³ä¹+å·¥ä½œã€é€šå‹¤+å­¦ä¹ æ’­å®¢ï¼‰
- **å†²çªä¸¥é‡ç¨‹åº¦**ï¼šcriticalï¼ˆå®Œå…¨å†²çªï¼‰/ highï¼ˆéœ€è¦è°ƒæ•´ï¼‰/ mediumï¼ˆå¯åè°ƒï¼‰/ lowï¼ˆä¼ªå†²çªï¼‰
- **åŸå› **ï¼šç»“åˆäº‹ä»¶æ€§è´¨å’Œç”¨æˆ·åå¥½è¯´æ˜
- **å»ºè®®**ï¼šå…·ä½“çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«æ—¶é—´å»ºè®®

**é‡è¦æç¤º**ï¼š
- å¦‚æœå†²çªæ¶‰åŠä»–äººçš„åªè¯»æ—¥ç¨‹ï¼ˆæ ‡è®°ä¸º"åªè¯»"ï¼‰ï¼Œæ— æ³•ä¿®æ”¹è¯¥æ—¥ç¨‹ï¼Œåªèƒ½å»ºè®®è°ƒæ•´ç”¨æˆ·è‡ªå·±çš„æ—¥ç¨‹
- å¦‚æœå†²çªæ˜¯ç”¨æˆ·è‡ªå·±çš„ä¸¤ä¸ªæ—¥ç¨‹ï¼Œå¯ä»¥å»ºè®®è°ƒæ•´ä»»ä¸€æ—¥ç¨‹

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```
å†²çª1: çœŸå®å†²çª [HIGH]
- æ¶‰åŠ: ç”¨æˆ·æ—¥ç¨‹ + ä»–äººåªè¯»æ—¥ç¨‹
- åŸå› : ä¸¤ä¸ªéƒ½æ˜¯éœ€è¦å…¨ç¥è´¯æ³¨çš„ä¼šè®®
- å»ºè®®: ã€åªèƒ½è°ƒæ•´ç”¨æˆ·æ—¥ç¨‹ã€‘å°† #Xã€Šç”¨æˆ·çš„ä¼šè®®ã€‹æ¨è¿Ÿåˆ° 15:30ï¼Œæˆ–æ”¹åˆ°æ¬¡æ—¥ä¸Šåˆ
```
""")
    
    if 'density' in analysis_focus:
        prompt_parts.append("""
## 2. å·¥ä½œå¯†åº¦åˆ†æ
å¯¹äºæ ‡è®°ä¸º"è¾ƒé«˜"æˆ–"è¿‡è½½"çš„æ—¥æœŸï¼Œè¯·åˆ†æï¼š
- å…·ä½“çš„é—®é¢˜ç‚¹ï¼ˆå¦‚è¿ç»­å·¥ä½œæ— ä¼‘æ¯ã€ç¼ºå°‘åˆé¤æ—¶é—´ç­‰ï¼‰
- ç»“åˆç”¨æˆ·åå¥½è¯„ä¼°ï¼ˆå¦‚ç”¨æˆ·æåˆ°çš„å·¥ä½œæ—¶é•¿åå¥½ï¼‰
- ç»™å‡ºå…·ä½“çš„ä¼˜åŒ–å»ºè®®

""")
    
    if 'reasonability' in analysis_focus:
        prompt_parts.append("""
## 3. åˆç†æ€§å®¡æŸ¥
æ£€æŸ¥ä»¥ä¸‹é—®é¢˜ï¼š
- æ·±å¤œæˆ–å‡Œæ™¨çš„äº‹ä»¶ï¼ˆç»“åˆç”¨æˆ·ä½œæ¯ä¹ æƒ¯ï¼‰
- è¶…é•¿äº‹ä»¶ï¼ˆ>4å°æ—¶æ— ä¼‘æ¯ï¼‰
- å‘¨æœ«å·¥ä½œï¼ˆç»“åˆç”¨æˆ·å·¥ä½œç”Ÿæ´»å¹³è¡¡åå¥½ï¼‰
- å…¶ä»–ä¸ç¬¦åˆç”¨æˆ·ä¹ æƒ¯çš„å®‰æ’

å¯¹æ¯ä¸ªé—®é¢˜ï¼Œè¯´æ˜åŸå› å¹¶ç»™å‡ºå…·ä½“å»ºè®®ã€‚
""")
    
    # 6. è¾“å‡ºè¦æ±‚
    prompt_parts.append("""
---

# è¾“å‡ºè¦æ±‚
1. ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼
2. æ¯æ¡å»ºè®®è¦å…·ä½“ã€å¯æ‰§è¡Œï¼ˆåŒ…å« #åºå·ã€å…·ä½“æ—¶é—´ï¼‰
3. ä¼˜å…ˆçº§æ’åºï¼šcritical > high > medium > low
4. ç»“åˆç”¨æˆ·ä¸ªäººåå¥½ç»™å‡ºä¸ªæ€§åŒ–å»ºè®®
5. å¯¹äºåªè¯»æ—¥ç¨‹ï¼ˆä»–äººåˆ†äº«ï¼‰ï¼Œè¯´æ˜æ— æ³•ä¿®æ”¹ä½†å¯ä»¥è°ƒæ•´è‡ªå·±çš„å®‰æ’
6. åœ¨æœ€åç»™å‡ºä¸€ä¸ªç®€æ´çš„æ€»ç»“å’Œä¼˜å…ˆå¤„ç†äº‹é¡¹

è¯·å¼€å§‹åˆ†æï¼š
""")
    
    return "\n".join(prompt_parts)


def _format_event_for_analysis(event: dict) -> str:
    """æ ¼å¼åŒ–å•ä¸ªäº‹ä»¶ä¾› LLM åˆ†æ"""
    index = event.get('_index', '?')
    title = event.get('title', 'æœªå‘½å')
    start = event.get('start', '')
    end = event.get('end', '')
    description = event.get('description', '')
    share_group = event.get('_share_group_name', '')
    owner = event.get('_owner_username', '')
    editable = event.get('_editable', True)
    source = event.get('_source', 'user')
    
    # æ„å»ºæ ‡é¢˜è¡Œ
    if source == 'share_group':
        parts = [f"**#{index}** ã€Š{title}ã€‹ [ä»–äººæ—¥ç¨‹]"]
    else:
        parts = [f"**#{index}** ã€Š{title}ã€‹"]
    
    parts.append(f"  - æ—¶é—´: {start} ~ {end}")
    
    if description:
        # æˆªæ–­è¿‡é•¿çš„æè¿°
        desc = description[:100] + '...' if len(description) > 100 else description
        parts.append(f"  - æè¿°: {desc}")
    
    if share_group:
        parts.append(f"  - æ¥æº: åˆ†äº«ç»„ã€Œ{share_group}ã€")
        if owner:
            parts.append(f"  - åˆ›å»ºè€…: {owner}")
    
    if not editable:
        parts.append(f"  - âš ï¸ **åªè¯»**ï¼ˆä»–äººæ—¥ç¨‹ï¼Œæ— æ³•ä¿®æ”¹ï¼Œåªèƒ½è°ƒæ•´è‡ªå·±çš„å®‰æ’ï¼‰")
    else:
        parts.append(f"  - âœï¸ å¯ç¼–è¾‘")
    
    return "\n".join(parts)


# ==========================================
# LLM æ™ºèƒ½åˆ†æ
# ==========================================

def analyze_with_llm(
    user,
    events: List[dict],
    hard_conflicts: List[dict],
    personal_info: List[dict],
    daily_density: Dict[str, dict],
    analysis_focus: List[str]
) -> Tuple[str, dict]:
    """
    è°ƒç”¨ LLM è¿›è¡Œæ™ºèƒ½åˆ†æ
    
    Args:
        user: Django User å¯¹è±¡
        events: äº‹ä»¶åˆ—è¡¨
        hard_conflicts: ç¡¬å†²çªåˆ—è¡¨
        personal_info: ä¸ªäººåå¥½åˆ—è¡¨
        daily_density: æ¯æ—¥å¯†åº¦åˆ†æ
        analysis_focus: åˆ†æé‡ç‚¹
    
    Returns:
        (åˆ†æç»“æœæ–‡æœ¬, tokenä½¿ç”¨ä¿¡æ¯)
    """
    from langchain_openai import ChatOpenAI
    from agent_service.agent_graph import get_user_llm
    from agent_service.context_optimizer import (
        get_current_model_config, update_token_usage
    )
    
    # æ„å»ºæç¤ºè¯
    prompt = build_analysis_prompt(
        events, hard_conflicts, personal_info, daily_density, analysis_focus
    )
    
    # è·å–ç”¨æˆ·é…ç½®çš„ LLM
    try:
        user_llm = get_user_llm(user)
        current_model_id, _ = get_current_model_config(user)
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ· LLM é…ç½®å¤±è´¥: {e}")
        return f"æ— æ³•è·å– LLM é…ç½®: {e}", {}
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—¥ç¨‹åˆ†æå¸ˆï¼Œæ“…é•¿ï¼š
1. è¯†åˆ«çœŸå®çš„æ—¶é—´å†²çªï¼ˆè€ƒè™‘äº‹ä»¶æ€§è´¨ï¼Œæœ‰äº›äº‹æƒ…å¯ä»¥åŒæ—¶è¿›è¡Œï¼‰
2. è¯„ä¼°å·¥ä½œè´Ÿè·å’Œæ—¶é—´å®‰æ’çš„åˆç†æ€§
3. ç»“åˆç”¨æˆ·ä¸ªäººåå¥½ç»™å‡ºä¸ªæ€§åŒ–å»ºè®®

ä½ çš„åˆ†æè¦å®¢è§‚ã€å®ç”¨ï¼Œå»ºè®®è¦å…·ä½“ã€å¯æ‰§è¡Œï¼ˆåŒ…å« #åºå· å’Œå…·ä½“æ—¶é—´ï¼‰ã€‚
ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼è¾“å‡ºã€‚"""
    
    try:
        # è°ƒç”¨ LLM
        from langchain_core.messages import SystemMessage, HumanMessage
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=prompt)
        ]
        
        response = user_llm.invoke(messages)
        
        # è·å– Token ä½¿ç”¨é‡
        input_tokens = 0
        output_tokens = 0
        
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            if isinstance(usage, dict):
                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_tokens', 0)
                output_tokens = usage.get('output_tokens', 0) or usage.get('completion_tokens', 0)
            else:
                input_tokens = getattr(usage, 'input_tokens', 0) or getattr(usage, 'prompt_tokens', 0)
                output_tokens = getattr(usage, 'output_tokens', 0) or getattr(usage, 'completion_tokens', 0)
        
        # å›é€€ï¼šä» response_metadata è·å–
        if not input_tokens and hasattr(response, 'response_metadata'):
            metadata = response.response_metadata
            usage = metadata.get('token_usage') or metadata.get('usage') or {}
            input_tokens = usage.get('prompt_tokens', 0) or usage.get('input_tokens', 0)
            output_tokens = usage.get('completion_tokens', 0) or usage.get('output_tokens', 0)
        
        # å¦‚æœä»æ— æ³•è·å–ï¼Œä½¿ç”¨ä¼°ç®—
        if input_tokens == 0:
            input_tokens = int(len(prompt) / 2.5)
        if output_tokens == 0:
            output_tokens = int(len(response.content) / 2.5) if hasattr(response, 'content') else 50
        
        # æ›´æ–° Token ç»Ÿè®¡ï¼ˆè‡ªåŠ¨è®¡è´¹ï¼‰
        if input_tokens > 0 or output_tokens > 0:
            update_token_usage(user, input_tokens, output_tokens, current_model_id)
            logger.info(f"[å†²çªåˆ†æ] Token å·²ç»Ÿè®¡: in={input_tokens}, out={output_tokens}, model={current_model_id}")
        
        token_info = {
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'model_id': current_model_id
        }
        
        # æå–åˆ†æç»“æœæ–‡æœ¬
        if hasattr(response, 'content'):
            content = response.content
            if isinstance(content, str):
                analysis_result = content
            elif isinstance(content, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼ˆå¤šä¸ªå†…å®¹å—ï¼‰ï¼Œæå–æ–‡æœ¬éƒ¨åˆ†
                text_parts = []
                for item in content:
                    if isinstance(item, str):
                        text_parts.append(item)
                    elif isinstance(item, dict) and 'text' in item:
                        text_parts.append(item['text'])
                analysis_result = "\n".join(text_parts)
            else:
                analysis_result = str(content)
        else:
            analysis_result = str(response)
        
        return analysis_result, token_info
        
    except Exception as e:
        logger.exception(f"LLM åˆ†æå¤±è´¥: {e}")
        return f"LLM åˆ†æå¤±è´¥: {e}", {}


# ==========================================
# æ ¼å¼åŒ–å†²çªæŠ¥å‘Š
# ==========================================

def format_hard_conflicts_report(conflicts: List[dict]) -> str:
    """æ ¼å¼åŒ–ç¡¬å†²çªæ£€æµ‹æŠ¥å‘Š"""
    if not conflicts:
        return "âœ… æœªæ£€æµ‹åˆ°æ—¶é—´é‡å "
    
    lines = [f"æ£€æµ‹åˆ° {len(conflicts)} ä¸ªæ—¶é—´é‡å ï¼š\n"]
    
    for conflict in conflicts:
        e1, e2 = conflict['events']
        idx1 = e1.get('_index', '?')
        idx2 = e2.get('_index', '?')
        title1 = e1.get('title', 'æœªå‘½å')
        title2 = e2.get('title', 'æœªå‘½å')
        
        lines.append(f"ã€å†²çª{conflict['conflict_id']}ã€‘#{idx1}ã€Š{title1}ã€‹ vs #{idx2}ã€Š{title2}ã€‹")
        lines.append(f"  é‡å æ—¶æ®µ: {conflict['overlap_period']['start']} ~ {conflict['overlap_period']['end']} ({conflict['overlap_duration']}åˆ†é’Ÿ)")
        lines.append("")
    
    return "\n".join(lines)


def format_density_report(daily_density: Dict[str, dict]) -> str:
    """æ ¼å¼åŒ–å·¥ä½œå¯†åº¦æŠ¥å‘Š"""
    if not daily_density:
        return "æ— å·¥ä½œå¯†åº¦æ•°æ®"
    
    lines = []
    for date_str, density in sorted(daily_density.items()):
        lines.append(
            f"{density['load_emoji']} **{date_str} ({density['weekday']})**: "
            f"{density['event_count']} ä¸ªäº‹ä»¶ï¼Œå…± {density['total_hours']} å°æ—¶ - {density['load_text']}"
        )
    
    return "\n".join(lines)
