"""
Agent WebSocket Consumer
å¤„ç†ä¸ Agent çš„å®æ—¶é€šä¿¡
"""
import json
import asyncio
import logging
from typing import Optional
from channels.generic.websocket import AsyncWebsocketConsumer
from channels.db import database_sync_to_async
from asgiref.sync import sync_to_async
from langchain_core.messages import HumanMessage, AIMessage, AIMessageChunk, ToolMessage
from django.contrib.auth.models import User

from logger import logger

class AgentConsumer(AsyncWebsocketConsumer):
    """
    Agent WebSocket Consumer
    
    URL: ws://host/ws/agent/?active_experts=planner,chat&session_id=xxx
    
    æ¶ˆæ¯åè®®:
    - å®¢æˆ·ç«¯ -> æœåŠ¡å™¨:
        {"type": "message", "content": "ç”¨æˆ·æ¶ˆæ¯"}
        {"type": "ping"}
    
    - æœåŠ¡å™¨ -> å®¢æˆ·ç«¯:
        {"type": "token", "content": "æµå¼token"}
        {"type": "message", "content": "å®Œæ•´æ¶ˆæ¯", "finished": true}
        {"type": "tool_call", "name": "tool_name", "args": {...}}
        {"type": "tool_result", "name": "tool_name", "result": "..."}
        {"type": "error", "message": "é”™è¯¯ä¿¡æ¯"}
        {"type": "pong"}
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.user: Optional[User] = None
        self.session_id: Optional[str] = None
        self.active_tools: list = []  # å¯ç”¨çš„å·¥å…·åˆ—è¡¨
        self.graph = None
        self.is_processing = False
        self.should_stop = False  # åœæ­¢æ ‡å¿—
        self.current_task: Optional[asyncio.Task] = None  # å½“å‰å¤„ç†ä»»åŠ¡
    
    async def connect(self):
        """å¤„ç† WebSocket è¿æ¥"""
        # 1. éªŒè¯ç”¨æˆ·èº«ä»½
        self.user = self.scope.get("user")
        if not self.user or not self.user.is_authenticated:
            logger.warning("æœªè®¤è¯ç”¨æˆ·å°è¯•è¿æ¥ WebSocket")
            await self.close(code=4001)
            return
        
        # 2. è§£æ URL å‚æ•°
        query_string = self.scope.get("query_string", b"").decode()
        params = self._parse_query_string(query_string)
        
        self.session_id = params.get("session_id", f"user_{self.user.id}_default")
        
        # è·å–å¯ç”¨çš„å·¥å…·ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤å·¥å…·ï¼‰
        from agent_service.agent_graph import get_default_tools
        tools_param = params.get("active_tools", "")
        if tools_param:
            self.active_tools = [t.strip() for t in tools_param.split(",") if t.strip()]
        else:
            self.active_tools = get_default_tools()
        
        logger.debug(f"[WebSocket] ç”¨æˆ· {self.user.username} è¿æ¥å‚æ•°:")
        logger.debug(f"[WebSocket]   - session_id: {self.session_id}")
        logger.debug(f"[WebSocket]   - tools_param: '{tools_param}'")
        logger.debug(f"[WebSocket]   - active_tools è§£æç»“æœ: {self.active_tools}")
        logger.info(f"ç”¨æˆ· {self.user.username} è¿æ¥ WebSocket, session={self.session_id}, tools={len(self.active_tools)} ä¸ª")
        
        # 3. åˆå§‹åŒ– Agent Graph
        await self._init_graph()
        
        # 4. æ¥å—è¿æ¥
        await self.accept()
        
        # 5. åŠ å…¥ session groupï¼ˆç”¨äºå¹¿æ’­æ¶ˆæ¯ï¼‰
        self.group_name = f"agent_session_{self.session_id}"
        await self.channel_layer.group_add(self.group_name, self.channel_name)
        logger.info(f"âœ… å·²åŠ å…¥ channel group: {self.group_name}")
        
        # 6. è·å–å½“å‰æ¶ˆæ¯æ•°é‡
        current_message_count = 0
        try:
            from agent_service.agent_graph import app
            config = {"configurable": {"thread_id": self.session_id}}
            state = await sync_to_async(app.get_state)(config)
            if state and state.values:
                messages = state.values.get("messages", [])
                current_message_count = len(messages)
                logger.debug(f"[WebSocket] å½“å‰ä¼šè¯æ¶ˆæ¯æ•°: {current_message_count}")
        except Exception as e:
            logger.warning(f"[WebSocket] è·å–æ¶ˆæ¯æ•°é‡å¤±è´¥: {e}")
        
        # 6. å‘é€æ¬¢è¿æ¶ˆæ¯ï¼ˆåŒ…å«æ¶ˆæ¯æ•°é‡ï¼‰
        await self.send_json({
            "type": "connected",
            "session_id": self.session_id,
            "active_tools": self.active_tools,
            "message_count": current_message_count,
            "message": f"æ¬¢è¿, {self.user.username}!"
        })
    
    async def send_json(self, content, close=False):
        """é‡å†™ send_jsonï¼Œä½¿ç”¨ channel_layer å¹¿æ’­æ¶ˆæ¯åˆ°æ‰€æœ‰è¿æ¥"""
        # å¦‚æœæ˜¯æµå¼ç›¸å…³æ¶ˆæ¯ï¼Œé€šè¿‡ channel_layer å¹¿æ’­
        msg_type = content.get("type")
        if msg_type in ["stream_start", "stream_chunk", "tool_call", "tool_result", "stream_end", "finished"]:
            # é€šè¿‡ channel_layer å¹¿æ’­åˆ° group ä¸­æ‰€æœ‰è¿æ¥
            if hasattr(self, 'group_name') and self.channel_layer:
                await self.channel_layer.group_send(
                    self.group_name,
                    {
                        "type": "broadcast_message",
                        "content": content
                    }
                )
                logger.debug(f"ğŸ“¡ å¹¿æ’­æµå¼æ¶ˆæ¯: group={self.group_name}, type={msg_type}")
            else:
                # æ²¡æœ‰ channel_layerï¼Œå›é€€åˆ°ç›´æ¥å‘é€
                await self.send(text_data=json.dumps(content, ensure_ascii=False))
        else:
            # éæµå¼æ¶ˆæ¯ï¼Œç›´æ¥å‘é€
            await self.send(text_data=json.dumps(content, ensure_ascii=False))
    
    async def broadcast_message(self, event):
        """æ¥æ”¶ channel_layer å¹¿æ’­çš„æ¶ˆæ¯å¹¶å‘é€ç»™å®¢æˆ·ç«¯"""
        content = event["content"]
        await self.send(text_data=json.dumps(content, ensure_ascii=False))
        logger.debug(f"ğŸ“¥ è½¬å‘å¹¿æ’­æ¶ˆæ¯åˆ°å®¢æˆ·ç«¯: type={content.get('type')}")
    
    async def disconnect(self, close_code):
        """å¤„ç† WebSocket æ–­å¼€"""
        logger.info(f"ç”¨æˆ· {self.user.username if self.user else 'unknown'} æ–­å¼€ WebSocket, code={close_code}")
        
        # ç¦»å¼€ channel group
        if hasattr(self, 'group_name'):
            await self.channel_layer.group_discard(self.group_name, self.channel_name)
            logger.info(f"ğŸšª å·²ç¦»å¼€ channel group: {self.group_name}")
    
    async def receive(self, text_data):
        """å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯"""
        try:
            data = json.loads(text_data)
            msg_type = data.get("type", "message")
            
            if msg_type == "ping":
                # å¿ƒè·³å“åº”
                await self.send_json({"type": "pong"})
                
            elif msg_type == "stop":
                # åœæ­¢å½“å‰å¤„ç†
                if self.is_processing:
                    self.should_stop = True
                    # å–æ¶ˆå½“å‰ä»»åŠ¡
                    if self.current_task and not self.current_task.done():
                        self.current_task.cancel()
                        logger.info(f"ç”¨æˆ· {self.user.username} å–æ¶ˆäº†å½“å‰ä»»åŠ¡")
                    logger.info(f"ç”¨æˆ· {self.user.username} è¯·æ±‚åœæ­¢å¤„ç†")
                    await self.send_json({"type": "stopped", "message": "å·²åœæ­¢ç”Ÿæˆ"})
                else:
                    await self.send_json({"type": "info", "message": "å½“å‰æ²¡æœ‰æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯"})
                
            elif msg_type == "message":
                # ç”¨æˆ·æ¶ˆæ¯
                content = data.get("content", "").strip()
                if not content:
                    await self.send_json({"type": "error", "message": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"})
                    return
                
                if self.is_processing:
                    await self.send_json({"type": "error", "message": "æ­£åœ¨å¤„ç†ä¸Šä¸€æ¡æ¶ˆæ¯ï¼Œè¯·ç¨å€™"})
                    return
                
                # é‡ç½®åœæ­¢æ ‡å¿—
                self.should_stop = False
                # åˆ›å»ºä»»åŠ¡å¹¶è¿è¡Œ
                self.current_task = asyncio.create_task(self._process_message(content))
            
            elif msg_type == "continue":
                # ç”¨æˆ·é€‰æ‹©ç»§ç»­æ‰§è¡Œï¼ˆè¾¾åˆ°é€’å½’é™åˆ¶åï¼‰
                logger.info(f"ç”¨æˆ· {self.user.username} é€‰æ‹©ç»§ç»­æ‰§è¡Œ")
                if self.is_processing:
                    await self.send_json({"type": "error", "message": "æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å€™"})
                    return
                
                # é‡ç½®åœæ­¢æ ‡å¿—å¹¶ç»§ç»­
                self.should_stop = False
                self.current_task = asyncio.create_task(self._continue_processing())
            
            elif msg_type == "check_status":
                # æŸ¥è¯¢å½“å‰ä¼šè¯çš„å¤„ç†çŠ¶æ€ï¼ˆç”¨äºåˆ·æ–°åæ¢å¤æµå¼çŠ¶æ€ï¼‰
                logger.info(f"ç”¨æˆ· {self.user.username} æŸ¥è¯¢ä¼šè¯çŠ¶æ€")
                
                # è·å– LangGraph çš„å®é™…çŠ¶æ€
                has_pending_messages = False
                last_message_role = None
                should_sync_immediately = False
                try:
                    from agent_service.agent_graph import app
                    config = {"configurable": {"thread_id": self.session_id}}
                    state = await sync_to_async(app.get_state)(config)
                    if state and state.values:
                        messages = state.values.get("messages", [])
                        if messages:
                            last_msg = messages[-1]
                            last_message_role = getattr(last_msg, 'type', None)
                            # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å·¥å…·æ¶ˆæ¯æˆ–äººç±»æ¶ˆæ¯ï¼Œè¯´æ˜è¿˜éœ€è¦ç»§ç»­å¤„ç†
                            if last_message_role in ['tool', 'human']:
                                has_pending_messages = True
                            # å¦‚æœæœ€åä¸€æ¡æ˜¯ AI æ¶ˆæ¯ä¸”ä¸åœ¨å¤„ç†ä¸­ï¼Œè¯´æ˜æµå¼è¾“å‡ºå·²å®Œæˆï¼Œå‰ç«¯åº”è¯¥ç«‹å³åŒæ­¥
                            elif last_message_role == 'ai' and not self.is_processing:
                                should_sync_immediately = True
                                logger.info(f"æ£€æµ‹åˆ°æµå¼è¾“å‡ºå·²å®Œæˆï¼Œå»ºè®®å‰ç«¯ç«‹å³åŒæ­¥")
                except Exception as e:
                    logger.warning(f"è·å– LangGraph çŠ¶æ€å¤±è´¥: {e}")
                
                await self.send_json({
                    "type": "status_response",
                    "session_id": self.session_id,
                    "is_processing": self.is_processing,
                    "should_stop": self.should_stop,
                    "has_pending_messages": has_pending_messages,
                    "last_message_role": last_message_role,
                    "should_sync_immediately": should_sync_immediately
                })
                
            else:
                await self.send_json({"type": "error", "message": f"æœªçŸ¥æ¶ˆæ¯ç±»å‹: {msg_type}"})
                
        except json.JSONDecodeError:
            await self.send_json({"type": "error", "message": "æ— æ•ˆçš„ JSON æ ¼å¼"})
        except Exception as e:
            logger.exception(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            await self.send_json({"type": "error", "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"})
    
    async def _process_message(self, content: str):
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶çœŸæ­£æµå¼è¾“å‡º
        ä½¿ç”¨ stream_mode="messages" è·å– token çº§åˆ«çš„æµå¼è¾“å‡º
        """
        self.is_processing = True
        
        try:
            # é€šçŸ¥å¼€å§‹å¤„ç†
            await self.send_json({"type": "processing", "message": "æ­£åœ¨æ€è€ƒ..."})
            
            # å‡†å¤‡é…ç½®
            config = {
                "configurable": {
                    "thread_id": self.session_id,
                    "user": self.user,
                    "active_tools": self.active_tools  # ä¼ é€’ active_tools åˆ° config
                }
            }
            
            # å¯¼å…¥ graph
            from agent_service.agent_graph import app
            
            # ========== ã€å…³é”®ã€‘å‘é€å‰æ£€æŸ¥å¹¶æ‰§è¡Œå†å²æ€»ç»“ ==========
            # è·å–å½“å‰å†å²æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“
            try:
                current_state = await sync_to_async(app.get_state)(config)
                if current_state and current_state.values:
                    current_messages = current_state.values.get("messages", [])
                    if current_messages:
                        await self._check_and_summarize(current_messages, config)
            except Exception as e:
                logger.warning(f"[æ€»ç»“] å‘é€å‰æ£€æŸ¥å¤±è´¥: {e}")
            
            # å‡†å¤‡è¾“å…¥
            input_state = {
                "messages": [HumanMessage(content=content)],
                "active_tools": self.active_tools
            }
            
            logger.debug(f"[æ¶ˆæ¯å¤„ç†] å‡†å¤‡è¾“å…¥çŠ¶æ€:")
            logger.debug(f"[æ¶ˆæ¯å¤„ç†]   - ç”¨æˆ·æ¶ˆæ¯: {content[:100]}...")
            logger.debug(f"[æ¶ˆæ¯å¤„ç†]   - active_tools (input_state): {self.active_tools}")
            logger.debug(f"[æ¶ˆæ¯å¤„ç†]   - active_tools (config): {config['configurable']['active_tools']}")
            
            # ä½¿ç”¨ queue åœ¨çº¿ç¨‹å’Œå¼‚æ­¥ä»£ç ä¹‹é—´ä¼ é€’äº‹ä»¶
            import queue
            import threading
            
            event_queue = queue.Queue()
            
            def run_stream():
                """
                åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„ stream
                ä½¿ç”¨é»˜è®¤ stream æ¨¡å¼è·å–èŠ‚ç‚¹çº§åˆ«çš„è¾“å‡º
                """
                try:
                    print(f"[Stream] å¼€å§‹æµå¼å¤„ç†, input_state keys={input_state.keys()}")
                    chunk_count = 0
                    
                    # ä½¿ç”¨é»˜è®¤ stream æ¨¡å¼ (è¿”å› state updates)
                    for output in app.stream(input_state, config):
                        chunk_count += 1
                        print(f"[Stream] chunk #{chunk_count}: output_type={type(output)}")
                        
                        # output æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkey æ˜¯èŠ‚ç‚¹åç§°ï¼Œvalue æ˜¯è¯¥èŠ‚ç‚¹çš„è¾“å‡º
                        for node_name, node_output in output.items():
                            print(f"[Stream]   node={node_name}, output_type={type(node_output)}")
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰ messages
                            if isinstance(node_output, dict) and 'messages' in node_output:
                                for msg in node_output['messages']:
                                    print(f"[Stream]     msg_type={type(msg).__name__}")
                                    if hasattr(msg, 'content'):
                                        print(f"[Stream]     content={msg.content[:100] if msg.content else 'empty'}...")
                                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                        print(f"[Stream]     tool_calls={msg.tool_calls}")
                                    
                                    # æŠŠæ¶ˆæ¯æ”¾å…¥é˜Ÿåˆ—
                                    event_queue.put(("message", (node_name, msg)))
                            
                        if self.should_stop:
                            event_queue.put(("stop", None))
                            break
                            
                    print(f"[Stream] æµå¼å¤„ç†å®Œæˆ, å…± {chunk_count} ä¸ª outputs")
                    event_queue.put(("done", None))
                except Exception as e:
                    import traceback
                    error_str = str(e)
                    print(f"[Stream] æµå¼å¤„ç†å¼‚å¸¸: {e}")
                    traceback.print_exc()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é€’å½’é™åˆ¶é”™è¯¯
                    if "Recursion limit" in error_str or "GraphRecursionError" in type(e).__name__:
                        event_queue.put(("recursion_limit", error_str))
                    else:
                        event_queue.put(("error", f"{error_str}\n{traceback.format_exc()}"))
            
            # å¯åŠ¨åå°çº¿ç¨‹
            thread = threading.Thread(target=run_stream, daemon=True)
            thread.start()
            
            # æµå¼è¾“å‡ºçŠ¶æ€
            stream_started = False
            current_tool_calls = {}  # è¿½è¸ªå·¥å…·è°ƒç”¨
            
            # å¼‚æ­¥æ¶ˆè´¹é˜Ÿåˆ—
            while True:
                if self.should_stop:
                    if stream_started:
                        await self.send_json({"type": "stream_end"})
                    break
                
                try:
                    # éé˜»å¡æ£€æŸ¥é˜Ÿåˆ—
                    item_type, item = event_queue.get_nowait()
                    
                    if item_type == "done" or item_type == "stop":
                        # ç¡®ä¿æµç»“æŸ
                        if stream_started:
                            await self.send_json({"type": "stream_end"})
                            stream_started = False
                        break
                    elif item_type == "recursion_limit":
                        # è¾¾åˆ°é€’å½’é™åˆ¶ï¼Œé€šçŸ¥å‰ç«¯
                        logger.warning(f"è¾¾åˆ°é€’å½’é™åˆ¶: {item}")
                        if stream_started:
                            await self.send_json({"type": "stream_end"})
                            stream_started = False
                        await self.send_json({
                            "type": "recursion_limit",
                            "message": "å·¥å…·è°ƒç”¨æ¬¡æ•°è¾¾åˆ°ä¸Šé™ï¼Œæ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿ"
                        })
                        break
                    elif item_type == "error":
                        raise Exception(item)
                    elif item_type == "message":
                        # æ–°æ ¼å¼: (node_name, msg)
                        node_name, msg = item
                        
                        print(f"[Process] Processing message from node={node_name}, msg_type={type(msg).__name__}")
                        
                        # å¤„ç† AIMessage çš„å†…å®¹ï¼ˆæ— è®ºæ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éƒ½è¦æ˜¾ç¤ºï¼‰
                        if hasattr(msg, 'content') and msg.content:
                            content_preview = msg.content[:50] if len(msg.content) > 50 else msg.content
                            print(f"[Process] content present: {content_preview}...")
                            
                            # æ˜¾ç¤ºå†…å®¹ï¼ˆå³ä½¿æœ‰å·¥å…·è°ƒç”¨ä¹Ÿè¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼‰
                            if not stream_started:
                                await self.send_json({"type": "stream_start"})
                                stream_started = True
                            await self.send_json({
                                "type": "stream_chunk",
                                "content": msg.content
                            })
                        
                        # å¤„ç†å·¥å…·è°ƒç”¨
                        if hasattr(msg, 'tool_calls') and msg.tool_calls:
                            print(f"[Process] tool_calls present: {msg.tool_calls}")
                            # å¦‚æœæœ‰å†…å®¹æ­£åœ¨æµå¼è¾“å‡ºï¼Œå…ˆç»“æŸå®ƒ
                            if stream_started:
                                await self.send_json({"type": "stream_end"})
                                stream_started = False
                            for tc in msg.tool_calls:
                                await self.send_json({
                                    "type": "tool_call",
                                    "name": tc.get("name", "unknown"),
                                    "args": tc.get("args", {})
                                })
                        
                        # å¤„ç† ToolMessage
                        if hasattr(msg, 'type') and getattr(msg, 'type', None) == 'tool':
                            result_str = str(msg.content) if hasattr(msg, 'content') else str(msg)
                            await self.send_json({
                                "type": "tool_result",
                                "name": msg.name if hasattr(msg, 'name') else "tool",
                                "result": result_str[:200] + "..." if len(result_str) > 200 else result_str
                            })
                        
                except queue.Empty:
                    await asyncio.sleep(0.01)  # æ›´çŸ­çš„ç­‰å¾…æ—¶é—´ä»¥æé«˜å“åº”é€Ÿåº¦
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            thread.join(timeout=2.0)
            
            # ç¡®ä¿æµæ­£ç¡®ç»“æŸ
            if stream_started:
                await self.send_json({"type": "stream_end"})
            
            # è·å–æœ€ç»ˆæ¶ˆæ¯æ•°é‡
            final_message_count = 0
            final_messages = []
            try:
                final_state = await sync_to_async(app.get_state)(config)
                if final_state and final_state.values:
                    final_messages = final_state.values.get("messages", [])
                    final_message_count = len(final_messages)
            except Exception as e:
                logger.warning(f"è·å–æœ€ç»ˆæ¶ˆæ¯æ•°é‡å¤±è´¥: {e}")
            
            # å‘é€å®Œæˆä¿¡å·ï¼ˆåŒ…å«æ¶ˆæ¯æ•°é‡ï¼‰
            if not self.should_stop:
                await self.send_json({
                    "type": "finished",
                    "message": "å¤„ç†å®Œæˆ",
                    "message_count": final_message_count
                })
                
                # æ³¨æ„ï¼šæ€»ç»“æ£€æŸ¥å·²ç§»è‡³å‘é€å‰æ‰§è¡Œï¼Œå›å¤åä¸å†æ£€æŸ¥
            
        except asyncio.CancelledError:
            logger.info(f"æ¶ˆæ¯å¤„ç†ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.exception(f"Agent è°ƒç”¨å¤±è´¥: {e}")
            await self.send_json({
                "type": "error",
                "message": f"Agent è°ƒç”¨å¤±è´¥: {str(e)}"
            })
        finally:
            self.is_processing = False
            self.current_task = None
            self.should_stop = False

    async def _continue_processing(self):
        """
        ç»§ç»­å¤„ç†ï¼ˆè¾¾åˆ°é€’å½’é™åˆ¶åç”¨æˆ·é€‰æ‹©ç»§ç»­ï¼‰
        æ·»åŠ ä¸€æ¡ç»§ç»­æ‰§è¡Œçš„æ¶ˆæ¯ï¼Œè®© Agent ä»ä¸­æ–­å¤„ç»§ç»­
        """
        self.is_processing = True
        
        try:
            await self.send_json({"type": "processing", "message": "ç»§ç»­æ‰§è¡Œ..."})
            
            from agent_service.agent_graph import app
            from langchain_core.messages import HumanMessage
            
            config = {
                "configurable": {
                    "thread_id": self.session_id,
                    "user": self.user,
                    "active_tools": self.active_tools
                }
            }
            
            # å…ˆæ£€æŸ¥å¹¶æ¸…ç†ä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨
            await self._cleanup_incomplete_tool_calls(config)
            
            # æ„å»ºç»§ç»­æ‰§è¡Œçš„è¾“å…¥æ¶ˆæ¯
            continue_message = HumanMessage(content="ç»§ç»­æ‰§è¡Œä¸Šè¿°ä»»åŠ¡ï¼Œå®Œæˆæœªå®Œæˆçš„å·¥ä½œã€‚")
            input_state = {"messages": [continue_message]}
            
            import queue
            import threading
            
            event_queue = queue.Queue()
            
            def run_continue():
                """ç»§ç»­æ‰§è¡Œï¼Œå‘é€ä¸€æ¡ç»§ç»­æ¶ˆæ¯è§¦å‘ Agent"""
                try:
                    print(f"[Continue] å‘é€ç»§ç»­æ¶ˆæ¯è§¦å‘æ‰§è¡Œ")
                    chunk_count = 0
                    
                    # ä½¿ç”¨æ–°æ¶ˆæ¯è§¦å‘ Agent ç»§ç»­æ‰§è¡Œ
                    for output in app.stream(input_state, config):
                        chunk_count += 1
                        print(f"[Continue] chunk #{chunk_count}: output_type={type(output)}")
                        
                        for node_name, node_output in output.items():
                            if isinstance(node_output, dict) and 'messages' in node_output:
                                for msg in node_output['messages']:
                                    event_queue.put(("message", (node_name, msg)))
                            
                        if self.should_stop:
                            event_queue.put(("stop", None))
                            break
                            
                    print(f"[Continue] ç»§ç»­æ‰§è¡Œå®Œæˆ, å…± {chunk_count} ä¸ª outputs")
                    event_queue.put(("done", None))
                except Exception as e:
                    import traceback
                    error_str = str(e)
                    print(f"[Continue] ç»§ç»­æ‰§è¡Œå¼‚å¸¸: {e}")
                    traceback.print_exc()
                    
                    if "Recursion limit" in error_str or "GraphRecursionError" in type(e).__name__:
                        event_queue.put(("recursion_limit", error_str))
                    else:
                        event_queue.put(("error", f"{error_str}\n{traceback.format_exc()}"))
            
            thread = threading.Thread(target=run_continue, daemon=True)
            thread.start()
            
            stream_started = False
            
            while True:
                if self.should_stop:
                    if stream_started:
                        await self.send_json({"type": "stream_end"})
                    break
                
                try:
                    item_type, item = event_queue.get_nowait()
                    
                    if item_type == "done" or item_type == "stop":
                        if stream_started:
                            await self.send_json({"type": "stream_end"})
                            stream_started = False
                        break
                    elif item_type == "recursion_limit":
                        logger.warning(f"ç»§ç»­æ‰§è¡Œæ—¶å†æ¬¡è¾¾åˆ°é€’å½’é™åˆ¶: {item}")
                        if stream_started:
                            await self.send_json({"type": "stream_end"})
                            stream_started = False
                        await self.send_json({
                            "type": "recursion_limit",
                            "message": "å·¥å…·è°ƒç”¨æ¬¡æ•°å†æ¬¡è¾¾åˆ°ä¸Šé™ï¼Œæ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿ"
                        })
                        break
                    elif item_type == "error":
                        raise Exception(item)
                    elif item_type == "message":
                        node_name, msg = item
                        
                        if hasattr(msg, 'content') and msg.content:
                            has_tool_calls = hasattr(msg, 'tool_calls') and msg.tool_calls
                            if not has_tool_calls:
                                if not stream_started:
                                    await self.send_json({"type": "stream_start"})
                                    stream_started = True
                                await self.send_json({
                                    "type": "stream_chunk",
                                    "content": msg.content
                                })
                        
                        if hasattr(msg, 'tool_calls') and msg.tool_calls:
                            if stream_started:
                                await self.send_json({"type": "stream_end"})
                                stream_started = False
                            for tc in msg.tool_calls:
                                await self.send_json({
                                    "type": "tool_call",
                                    "name": tc.get("name", "unknown"),
                                    "args": tc.get("args", {})
                                })
                        
                        if hasattr(msg, 'type') and getattr(msg, 'type', None) == 'tool':
                            result_str = str(msg.content) if hasattr(msg, 'content') else str(msg)
                            await self.send_json({
                                "type": "tool_result",
                                "name": msg.name if hasattr(msg, 'name') else "tool",
                                "result": result_str[:200] + "..." if len(result_str) > 200 else result_str
                            })
                        
                except queue.Empty:
                    await asyncio.sleep(0.01)
            
            thread.join(timeout=2.0)
            
            if stream_started:
                await self.send_json({"type": "stream_end"})
            
            # è·å–æœ€ç»ˆæ¶ˆæ¯æ•°é‡
            final_message_count = 0
            try:
                final_state = await sync_to_async(app.get_state)(config)
                if final_state and final_state.values:
                    final_messages = final_state.values.get("messages", [])
                    final_message_count = len(final_messages)
            except Exception as e:
                logger.warning(f"è·å–æœ€ç»ˆæ¶ˆæ¯æ•°é‡å¤±è´¥: {e}")
            
            if not self.should_stop:
                await self.send_json({
                    "type": "finished",
                    "message": "å¤„ç†å®Œæˆ",
                    "message_count": final_message_count
                })
            
        except asyncio.CancelledError:
            logger.info(f"ç»§ç»­å¤„ç†ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.exception(f"ç»§ç»­å¤„ç†å¤±è´¥: {e}")
            await self.send_json({
                "type": "error",
                "message": f"ç»§ç»­å¤„ç†å¤±è´¥: {str(e)}"
            })
        finally:
            self.is_processing = False
            self.current_task = None
            self.should_stop = False

    async def _cleanup_incomplete_tool_calls(self, config):
        """
        æ¸…ç†ä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨æ¶ˆæ¯
        å½“é€’å½’é™åˆ¶ä¸­æ–­æ—¶ï¼Œå¯èƒ½å­˜åœ¨ AIMessage æœ‰ tool_calls ä½†æ²¡æœ‰å¯¹åº”çš„ ToolMessage
        """
        from agent_service.agent_graph import app
        from langchain_core.messages import AIMessage, ToolMessage
        
        try:
            state = await sync_to_async(app.get_state)(config)
            if not state or not state.values:
                return
            
            messages = state.values.get("messages", [])
            if not messages:
                return
            
            last_msg = messages[-1]
            
            # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯å¸¦æœ‰ tool_calls çš„ AIMessage
            if isinstance(last_msg, AIMessage) and hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                logger.info(f"æ£€æµ‹åˆ°ä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼Œéœ€è¦æ¸…ç†")
                
                # ä¸ºæ¯ä¸ªæœªå®Œæˆçš„ tool_call æ·»åŠ ä¸€ä¸ªå‡çš„ ToolMessage
                fake_tool_messages = []
                for tc in last_msg.tool_calls:
                    tool_call_id = tc.get("id", "")
                    tool_name = tc.get("name", "unknown")
                    fake_tool_messages.append(
                        ToolMessage(
                            content=f"[å·¥å…·è°ƒç”¨è¢«ä¸­æ–­ï¼Œæœªå®Œæˆæ‰§è¡Œ]",
                            tool_call_id=tool_call_id,
                            name=tool_name
                        )
                    )
                
                if fake_tool_messages:
                    # æ›´æ–°çŠ¶æ€æ·»åŠ å‡çš„å·¥å…·å“åº”
                    await sync_to_async(app.update_state)(
                        config, 
                        {"messages": fake_tool_messages}
                    )
                    logger.info(f"å·²æ·»åŠ  {len(fake_tool_messages)} ä¸ªå ä½å·¥å…·å“åº”")
                    
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸å®Œæ•´å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {e}")

    async def _init_graph(self):
        """åˆå§‹åŒ– Agent Graph"""
        from agent_service.agent_graph import app
        self.graph = app
    
    def _parse_query_string(self, query_string: str) -> dict:
        """è§£æ URL æŸ¥è¯¢å‚æ•°"""
        from urllib.parse import unquote
        params = {}
        if query_string:
            for pair in query_string.split("&"):
                if "=" in pair:
                    key, value = pair.split("=", 1)
                    # URL è§£ç å‚æ•°å€¼
                    params[key] = unquote(value)
        return params

    async def _check_and_summarize(self, messages, config):
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œå†å²æ€»ç»“ï¼Œå¦‚æœéœ€è¦åˆ™æ‰§è¡Œ
        
        Args:
            messages: å½“å‰æ‰€æœ‰æ¶ˆæ¯
            config: Graph é…ç½®
        """
        try:
            from agent_service.models import AgentSession
            from agent_service.context_summarizer import ConversationSummarizer
            from agent_service.context_optimizer import TokenCalculator, get_optimization_config
            from agent_service.agent_graph import get_user_llm, get_current_model_config
            
            session_id = config.get("configurable", {}).get("thread_id", "")
            if not session_id:
                return
            
            # è·å–ä¼˜åŒ–é…ç½®ï¼ˆæ·»åŠ è¯¦ç»†æ—¥å¿—ï¼‰
            logger.info(f"[æ€»ç»“] å¼€å§‹è·å–ä¼˜åŒ–é…ç½®, user={self.user}, user_id={self.user.id if self.user else 'None'}")
            opt_config = await database_sync_to_async(get_optimization_config)(self.user)
            logger.info(f"[æ€»ç»“] è·å–åˆ°çš„é…ç½®: {opt_config}")
            
            # è·å–ç”¨æˆ·é…ç½®çš„ LLMï¼ˆç”¨äºæ€»ç»“ï¼‰
            user_llm = await database_sync_to_async(get_user_llm)(self.user)
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ€»ç»“
            if not opt_config.get('enable_summarization', True):
                logger.debug("[æ€»ç»“] æ€»ç»“åŠŸèƒ½å·²ç¦ç”¨")
                return
            
            min_messages = opt_config.get('min_messages_before_summary', 20)
            if not messages or len(messages) < min_messages:
                logger.debug(f"[æ€»ç»“] æ¶ˆæ¯æ•°ä¸è¶³: {len(messages) if messages else 0} < {min_messages}")
                return
            
            # è·å–ä¼šè¯
            session = await database_sync_to_async(
                AgentSession.objects.filter(session_id=session_id).first
            )()
            if not session:
                return
            
            # è·å–ç°æœ‰æ€»ç»“
            summary_metadata = await database_sync_to_async(session.get_summary_metadata)()
            
            # Token è®¡ç®—å™¨
            calculator = TokenCalculator(
                method=opt_config.get('token_calculation_method', 'estimate')
            )
            
            # è·å–æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£
            _, model_config = await database_sync_to_async(get_current_model_config)(self.user)
            context_window = model_config.get('context_window', 128000) if model_config else 128000
            
            # åˆ›å»ºæ€»ç»“å™¨ï¼ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„ LLMï¼‰
            summarizer = ConversationSummarizer(
                llm=user_llm,
                token_calculator=calculator,
                context_window=context_window,
                target_usage_ratio=opt_config.get('target_usage_ratio', 0.6),
                summary_trigger_ratio=opt_config.get('summary_trigger_ratio', 0.5),
                min_messages_before_summary=min_messages,
                summary_token_ratio=opt_config.get('summary_token_ratio', 0.26),
                target_summary_tokens=opt_config.get('target_summary_tokens', 2000)
            )
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“
            if not summarizer.should_summarize(messages, summary_metadata):
                return
            
            logger.info(f"[æ€»ç»“] è§¦å‘å†å²æ€»ç»“: session={session_id}, messages={len(messages)}")
            
            # è®¾ç½®æ­£åœ¨æ€»ç»“çŠ¶æ€
            await database_sync_to_async(session.set_summarizing)(True)
            
            # é€šçŸ¥å‰ç«¯å¼€å§‹æ€»ç»“
            await self.send_json({
                "type": "summarizing_start",
                "message": "æ­£åœ¨æ€»ç»“å¯¹è¯å†å²..."
            })
            
            try:
                # è®¡ç®—éœ€è¦æ€»ç»“çš„èŒƒå›´
                start_idx, end_idx = summarizer.calculate_summarize_range(messages, summary_metadata)
                messages_to_summarize = messages[start_idx:end_idx]
                
                # è·å–ä¹‹å‰çš„æ€»ç»“ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
                previous_summary = summary_metadata.get('summary', '') if summary_metadata else None
                
                # æ‰§è¡Œæ€»ç»“
                new_summary_metadata = await summarizer.summarize(
                    messages_to_summarize,
                    previous_summary=previous_summary
                )
                
                if new_summary_metadata:
                    # ä¿å­˜æ€»ç»“
                    await database_sync_to_async(session.save_summary)(
                        summary_text=new_summary_metadata['summary'],
                        summarized_until=end_idx,
                        summary_tokens=new_summary_metadata['summary_tokens']
                    )
                    
                    logger.info(f"[æ€»ç»“] å®Œæˆ: æ€»ç»“äº† {end_idx} æ¡æ¶ˆæ¯, {new_summary_metadata['summary_tokens']} tokens")
                    
                    # é€šçŸ¥å‰ç«¯æ€»ç»“å®Œæˆ
                    await self.send_json({
                        "type": "summarizing_end",
                        "success": True,
                        "summary": new_summary_metadata['summary'],
                        "summarized_until": end_idx,
                        "summary_tokens": new_summary_metadata['summary_tokens']
                    })
                else:
                    await database_sync_to_async(session.set_summarizing)(False)
                    await self.send_json({
                        "type": "summarizing_end",
                        "success": False,
                        "message": "æ€»ç»“ç”Ÿæˆå¤±è´¥"
                    })
                    
            except Exception as e:
                logger.error(f"[æ€»ç»“] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                await database_sync_to_async(session.set_summarizing)(False)
                await self.send_json({
                    "type": "summarizing_end",
                    "success": False,
                    "message": f"æ€»ç»“å¤±è´¥: {str(e)}"
                })
                
        except Exception as e:
            logger.error(f"[æ€»ç»“] æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)


class AgentStreamConsumer(AgentConsumer):
    """
    æ”¯æŒæµå¼è¾“å‡ºçš„ Agent Consumer
    ä½¿ç”¨ LangGraph çš„ stream æ–¹æ³•å®ç°æ‰“å­—æœºæ•ˆæœ
    """
    
    async def _process_message(self, content: str):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶æµå¼è¾“å‡º"""
        self.is_processing = True
        
        try:
            await self.send_json({"type": "processing", "message": "æ­£åœ¨æ€è€ƒ..."})
            
            config = {
                "configurable": {
                    "thread_id": self.session_id,
                    "user": self.user
                }
            }
            
            input_state = {
                "messages": [HumanMessage(content=content)],
                "next": "",
                "active_experts": self.active_experts
            }
            
            # ä½¿ç”¨æµå¼è¾“å‡º
            full_response = ""
            async for event in self._stream_graph(input_state, config):
                event_type = event.get("type")
                
                if event_type == "token":
                    # æµå¼ token
                    token = event.get("content", "")
                    full_response += token
                    await self.send_json({
                        "type": "token",
                        "content": token
                    })
                    
                elif event_type == "tool_call":
                    # å·¥å…·è°ƒç”¨é€šçŸ¥
                    await self.send_json({
                        "type": "tool_call",
                        "name": event.get("name"),
                        "args": event.get("args", {})
                    })
                    
                elif event_type == "tool_result":
                    # å·¥å…·æ‰§è¡Œç»“æœ
                    await self.send_json({
                        "type": "tool_result",
                        "name": event.get("name"),
                        "result": event.get("result", "")
                    })
            
            # å‘é€å®Œæˆä¿¡å·
            await self.send_json({
                "type": "message",
                "content": full_response,
                "finished": True
            })
            
        except Exception as e:
            logger.exception(f"Agent æµå¼è°ƒç”¨å¤±è´¥: {e}")
            await self.send_json({
                "type": "error",
                "message": f"Agent è°ƒç”¨å¤±è´¥: {str(e)}"
            })
        finally:
            self.is_processing = False
    
    async def _stream_graph(self, input_state, config):
        """
        æµå¼è°ƒç”¨ Graph
        æ³¨æ„: è¿™éœ€è¦ graph æ”¯æŒæµå¼è¾“å‡º
        ç›®å‰ä½¿ç”¨ç®€åŒ–å®ç°ï¼Œåç»­å¯ä»¥æ‰©å±•ä¸ºçœŸæ­£çš„æµå¼
        """
        # ç®€åŒ–å®ç°ï¼šç›´æ¥è°ƒç”¨ invoke ç„¶åè¿”å›å®Œæ•´å“åº”
        result = await self._invoke_graph(input_state, config)
        
        if result and "messages" in result:
            for msg in reversed(result["messages"]):
                if isinstance(msg, AIMessage):
                    # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆåˆ†å—å‘é€ï¼‰
                    content = msg.content
                    chunk_size = 10  # æ¯æ¬¡å‘é€çš„å­—ç¬¦æ•°
                    
                    for i in range(0, len(content), chunk_size):
                        chunk = content[i:i+chunk_size]
                        yield {"type": "token", "content": chunk}
                        await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                    break
